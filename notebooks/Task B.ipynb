{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: Meta-Learning Perfomance Prediction\n",
    "\n",
    "In this task, you will use information on training parameters and metadata on multiple OpenML dataset to train a performance predictor that performs well even for unseen datasets. You are provided with config parameters and metafeatures for six datasets. The datasets are split into training datasets and test datasets and you should only train on the training datasets.\n",
    "\n",
    "For questions, you can contact zimmerl@informatik.uni-freiburg.de\n",
    "\n",
    "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications:\n",
    "\n",
    "* Data: six_datasets_lw.json\n",
    "* Number of datasets: 6\n",
    "* Training datasets: higgs, vehicle, adult, volkert\n",
    "* Test datasets: Fashion-MNIST, jasmine\n",
    "* Number of configurations: 2000\n",
    "* Available data: architecture parameters and hyperparameters, metafeatures \n",
    "* Target: final validation accuracy\n",
    "* Evaluation metric: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing\n",
    "\n",
    "Note: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ..\n",
    "#external\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#local\n",
    "sys.path.append(\"../\")\n",
    "from networks.MyMLP import MyMLP\n",
    "from func.train_eval import train_model, eval_model\n",
    "from func.load_data import prepare_dataloaders, load_data_from_file\n",
    "from func.NN_HPO import PyTorchWorker"
   ]
  },
  {
   "source": [
    "## Cuda config"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "source": [
    "## Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_metafeatures_train, y_train, X_val, X_metafeatures_val, y_val, X_test, X_metafeatures_test, y_test    =   load_data_from_file(\"cached/six_datasets_lw.json\", \"cached/metafeatures_6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print()\n",
    "print(\"Y_Train:\",y_train.shape)\n",
    "print(\"Y_val:\",y_val.shape)\n",
    "print(\"Y_Test:\",y_test.shape)\n",
    "print()\n",
    "print(\"X_metafeatures_train:\",X_metafeatures_train.shape)\n",
    "print(\"X_metafeatures_val:\" ,X_metafeatures_val.shape)\n",
    "print(\"X_metafeatures_test:\" , X_metafeatures_test.shape)"
   ]
  },
  {
   "source": [
    "## "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Prepare data\n",
    "## Preprocess data + Create dataloaders for the preprocessed data tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataloader = prepare_dataloaders(X_hp=X_train, X_mf=X_metafeatures_train, y= y_train, scaling=\"minmax\",batch_size=batch_size)\n",
    "validation_dataloader = prepare_dataloaders(X_hp=X_val, X_mf=X_metafeatures_val, y= y_val, scaling=\"minmax\",batch_size=batch_size)\n",
    "test_dataloader = prepare_dataloaders(X_hp=X_test, X_mf=X_metafeatures_test, y= y_test, scaling=\"minmax\",batch_size=batch_size)"
   ]
  },
  {
   "source": [
    "## Check the data in the tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x,  y in train_dataloader:\n",
    "    print(\"X- minibatched: \", x)\n",
    "    print(\"y- minibatched: \", y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "MyMLP = MyMLP(7, 0.8, 55, 1)\n",
    "print(\"Model:\")\n",
    "print(MyMLP.model)\n",
    "model = MyMLP.model\n",
    "epochs = 70\n",
    "optimizer  = optim.Adam(model.parameters(), lr=1e-05)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, eta_min=1e-05)\n",
    "criterion = nn.MSELoss()\n",
    "train_model(train_dataloader, validation_dataloader, epochs, model, optimizer, scheduler, criterion)\n",
    "eval_model(test_dataloader, model, criterion)\n",
    "'''"
   ]
  },
  {
   "source": [
    "## Custom neural network tuned with BOHB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "from hpbandster.optimizers import BOHB\n",
    "\n",
    "import logging\n",
    "import pickle\n",
    "logging.getLogger('hpbandster').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_dir = os.curdir\n",
    "# minimum budget that BOHB uses\n",
    "min_budget = 1\n",
    "# largest budget BOHB will use\n",
    "max_budget = 9\n",
    "worker = PyTorchWorker(run_id='0', input_size=55, output_size=1, train_loader= train_dataloader, validation_loader= validation_dataloader, test_loader= test_dataloader)\n",
    "cs = worker.get_configspace()\n",
    "config = cs.sample_configuration().get_dictionary()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = worker.compute(config=config, budget=min_budget, working_directory=working_dir)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = os.path.join(working_dir, 'bohb_result.pkl')\n",
    "nic_name = 'lo0'\n",
    "port = 0\n",
    "run_id = 'bohb_run_1'\n",
    "n_bohb_iterations = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Start a nameserver #####\n",
    "    \n",
    "    # get host\n",
    "    try:\n",
    "        host = hpns.nic_name_to_host(nic_name)\n",
    "    except ValueError as e:\n",
    "        host = \"localhost\"\n",
    "        print(e)\n",
    "        print(\"ValueError getting host from nic_name {}, \"\n",
    "              \"setting to localhost.\".format(nic_name))    \n",
    "    \n",
    "    ns = hpns.NameServer(run_id=run_id, host=host, port=port,\n",
    "                         working_directory=working_dir)\n",
    "    ns_host, ns_port = ns.start()\n",
    "\n",
    "    # Start local worker\n",
    "    w = PyTorchWorker(run_id=run_id, host=host, nameserver=ns_host,\n",
    "                      nameserver_port=ns_port, timeout=120, input_size=55, output_size=1, train_loader= train_dataloader, validation_loader= validation_dataloader, test_loader= test_dataloader)\n",
    "    w.run(background=True)\n",
    "\n",
    "    # Run an optimizer\n",
    "    bohb = BOHB(configspace=worker.get_configspace(),\n",
    "                run_id=run_id,\n",
    "                host=host,\n",
    "                nameserver=ns_host,\n",
    "                nameserver_port=ns_port,\n",
    "                min_budget=min_budget, max_budget=max_budget)\n",
    "\n",
    "    result = bohb.run(n_iterations=n_bohb_iterations)\n",
    "    print(\"Write result to file {}\".format(result_file))\n",
    "    with open(result_file, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "finally:\n",
    "    bohb.shutdown(shutdown_workers=True)\n",
    "    ns.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inc_id = result.get_incumbent_id()  # get config_id of incumbent (lowest loss)\n",
    "inc_run = result.get_runs_by_id(inc_id)[-1]  # get run with this config_id on highest budget\n",
    "best_error, best_model = inc_run.loss, inc_run.info['model']\n",
    "print(\"The best model (config_id {}) has the lowest final error with {:.4f}.\"\n",
    "      .format(inc_run.config_id, best_error))\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpbandster.visualization as hpvis\n",
    "\n",
    "all_runs = result.get_all_runs()\n",
    "id2conf = result.get_id2config_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "hpvis.finished_runs_over_time(all_runs, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hpvis.correlation_across_budgets(result, show=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpvis.losses_over_time(all_runs, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpvis.performance_histogram_model_vs_random(all_runs, id2conf, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}